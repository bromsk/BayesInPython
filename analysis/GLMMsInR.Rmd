---
title: "GLMMs in R"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  bookdown::html_document2:
    fig_caption: yes
    fig_height: 4
    fig_width: 6
    highlight: tango
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: true
    code_folding: hide
editor_options:
  chunk_output_type: console
---

```{r setwd, eval = F, echo = F}
setwd("analysis")
```
```{r setup, include=FALSE, message=F, warning=FALSE, results = "hide", echo=F}
knitr::opts_chunk$set(cache = T, message =F, warning = F, echo=F,
                      # dpi = 36, out.width="600px", out.height="600px")
  fig.width = 8,
  fig.height = 5,
  out.width = '100%')

# R libraries:
library(MASS)
library(lme4)
library(magrittr)
library(tidyverse)
library(ggmap)
# usethis::edit_r_environ()
register_google(Sys.getenv("ggmapKey"))

# to run Bayesian models. Set appropriate nCores for your machine
library(brms)
nCores = 6 # nCores -2


## Set colors:
# library(RColorBrewer)
# display.brewer.pal(11, "Spectral")
# brewer.pal(11, "Spectral")
mycolors =  c("#9E0142", "#762A83","#9970AB",  # "#5E4FA2",
              "#3288BD","#66C2A5", "#ABDDA4", "#D53E4F")
visualcols = c( "#D7191C", "#FDAE61", "#FFFFBF", "#A6D96A", "#1A9641")
trialcols = c("#4575B4", "#40004B","#762A83","#9970AB",  "#5AAE61",
               "#C2A5CF","#74ADD1", "#E7D4E8", "#ABD9E9",   "#00441B" )
prop_colors = c("#313695", "#ABD9E9", "#FFFFBF", "#FEE090", 
                 "#FDAE61", "#F46D43","#D73027", "#A50026")
#
```
``` {r functions}

getTR <- function (mod, coefs = -1) {
  b1 = summary(mod)$coef[coefs,1]
  seb1 = summary(mod)$coef[coefs,2]
  (TR = round(100 * (1- exp(b1)),1))  ## 69.9
  (higher = round(100 * (1 - exp(b1 - 1.96*seb1)),1)) ## 78.8
  (lower = round(100 * (1 - exp(b1 + 1.96*seb1)),1)) ## 57.0
  if (is.null(names(TR))) {
   data.frame(#Treatment = names(TR),
             TR = as.numeric(TR), 
             lowerCI = as.numeric(lower),
             upperCI = as.numeric(higher))   
  } else {
  data.frame(Treatment = names(TR),
             TR = as.numeric(TR), 
             lowerCI = as.numeric(lower),
             upperCI = as.numeric(higher))
  }
}

## NA = 0 in sum
`%+%` <- function(x, y)  mapply(sum, x, y, MoreArgs = list(na.rm = TRUE))
#


```
```{r readdatR}

datRinit = read.csv("../data/moths.csv")
datRinit %<>% dplyr::select(-X)
# (there is a select function in MASS package that sometimes overrides dplyr)
```

# Introduction

I have used R, in conjunction with Rmarkdown documents, in an RStudio GUI for over a decade. It is a beautiful setup, with great work flows and clean coding. It is perfect for non-production level analyses, visualizations, interactive documents, web apps, and reports.

However, the world seems to prefer Python. Perhaps it is because computer programmers rule the world more than statisticians and scientists. In that vein, I am writing this tutorial to fit Bayesian GLMM's and non-linear models in Python. 

All models are written mathematically and fit in both R and Python. Frequentist versions are fit when available as well as Bayesian versions. There are four chapters: GLMM's in R; GLMM's in Python; non-linear models in R; and non-linear models in Python.

This particular webpage/chapter fits GLMM's in R.

## Outline

* Introduce the data to be used throughout the tutorial.

* Fit generalized linear models (GLMs, here Poisson and negative binomial regressions). Models are fit in both frequentist and Bayesian frameworks to lay the groundwork for the more complicated models.

* Fit generalized linear models with random effects (GLMMs) to allow for correlations in space and time. Models are fit in both frequentist and Bayesian frameworks.

# Analysis objectives

How well does the treatment work?


At each location, there is one rice field that is a control group and one rice field that is treated with pheromones. **The objective is to determine how good the treatment is at reducing moth populations. This is measured as a reduction of moth counts in the traps in the treatment fields compared to the traps in the control fields.**

The performance metric of interest is trap reduction (TR), which is a derived parameter of the regression coefficient:

$$
TR = 100 - 100 e ^{(-\beta_1)}
$$

If trapping reduction were 100%, no moths would ever be caught in the treatment traps. However, some random moths will inevitably land in any trap. Instead, product performance is considered good if 90% trapping reduction is achieved, meaning that 90% fewer moths are caught in the treatment traps. When trapping reduction goes below 0%, the parameter no longer makes sense and cannot be interpreted.

It should also be noted that treatment performance may decline over time. Perhaps there is not enough pheromone to last for the entire season, or the ecosystem may change during the season in a way that somehow lessens product performance (e.g., the pheromones from the plants may change and interact with the product.)

# Data Exploration

We are analyzing moth count data collected from traps from rice fields in Indonesia. There are a total of 10 locations that are sampled throughout a field season. At each location, there is one treatment (PFP) field with 4 traps and one control field with 4 traps. These traps are sampled and reset approximately every 10 days for the entire growing season, but sometimes that interval varies.

It is expected that the treatment field (trt A) will catch fewer moths than the control fields. The research questions are: how much fewer moths are caught, i.e., what is the trapping reduction associated with the PFP traps? And, is the same trapping reduction (TR) observed for the entire season?

These data have a spatial and temporal component. Spatially, the traps have a certain proximity to each other within a location, and the locations may be clumped across the landscape. Temporally, the same traps are sampled over time. 

These data are based on the data from Iqbal et al. (2023) (link: https://jurnal.pei-pusat.org/index.php/jei/article/view/783), but have been heavily manipulated in terms of location coordinates, location names, and trap counts to preserve privacy rights. The results presented in this tutorial are NOT representative of true product performance.

Full citation:
Iqbal, M., Marman, M., Arintya, F., Broms, K. ., Clark, T., & Srigiriraju, L. . (2023). Mating disruption technology: An innovative tool for managing yellow stem borer (Scirpophaga incertulas Walker) of rice in Indonesia: Teknologi gangguan kawin: Inovasi untuk pengendalian penggerek batang kuning (Scirpophaga incertulas Walker) pada padi di Indonesia. Jurnal Entomologi Indonesia, 20(2), 129. 

## Moth counts

Here is what the data look like. Columns: Location, Treatment, TrapID, Longitude, Latitude, AssessmentNumber, and SamplingDate should be self-explanatory... TransplantDate is the date that the rice seedlings were transplanted into the field; DispInstallDate is the treatment installation date; TrapInstallDate is the trap installation date; "nYSB" are the moth counts (number of YSB moths); DaysOfCatch is the number of days since the trap was previously sampled  or the number of days since trap installation if it is assessment number 1; DATI is days after trap installation; DADI is days after dispenser installation; and DAT is days after transplant. Some of these columns will be discussed more later.

```{r}
# knitr::kable(head(datRinit, 20))

# check data values:
summary(datRinit)
lapply(datRinit, function(x) {
  if (is.character(x))
    unique(x)
})
datR <- datRinit %>%
  mutate(TransplantDate = ymd(TransplantDate),
         DispInstallDate = ymd(DispInstallDate),
         TrapInstallDate = ymd(TrapInstallDate),
         SamplingDate = ymd(SamplingDate)
         )  %>%
  mutate(mothsperday = nYSB / DaysOfCatch,
         TreatmentF = factor(Treatment), # if we need to fix levels in non-alpha order
         LocationF = as.factor(Location),
         logDaysOfCatch = log(DaysOfCatch),
         # DATscaled = as.numeric(scale(DAT)),
         SamplingDateC = as.character(SamplingDate),
         numericDate = as.numeric(SamplingDate),
         numericDate = numericDate - min(numericDate) + 1)


```

The data are the moth counts collected at each trap, within each location and throughout the season. Note how the y-axis changes for each plot in the figure.

```{r plotMoths, fig.width = 8, fig.height = 6}
mean_cts <- datR %>%
  group_by(Location, Treatment, TreatmentF,
           AssessmentNumber,
           SamplingDate, DATI) %>%
  summarize(mean_cts = mean(nYSB, na.rm = T),
            mean_mothsperday = mean(mothsperday)) %>%
  ungroup()
ggplot(datR,
       aes(DATI, mothsperday, color = TreatmentF)) +
  geom_jitter(height = 0, width = 0.75, alpha = 0.5) +
  # geom_line(data = mean_cts,
      # aes(DATI, mean_mothsperday, color = TreatmentF), size = 1.1) +
  geom_smooth(se=F) +
  facet_wrap(~Location, scales = "free_y") + 
  scale_color_manual(values = mycolors[]) +
  # scale_y_sqrt() +
  labs(title = "Male YSB moth counts for each location and trap",
       y = "Moth counts per trap per day",
       x = "DAI",
       color = "Treatment")
#
```

## Trap locations

Trial locations in relation to each other. Some trial locations are closer to each other than others.

```{r}

basemap <- get_googlemap(c(lon = mean(datR$Longitude, na.rm = T),
                           lat = mean(datR$Latitude, na.rm = T)),
                         color = "bw",
                         zoom = 7)
ggmap(basemap) +
  geom_point(data = datR,
             aes(Longitude, Latitude, color = Location), size = 2) +
  labs(title = "Trial locations") +
  scale_color_brewer(palette = "PuOr") 
#
```

Within each location, the treatment traps have a slightly different alignment:

```{r}
coords <- datR %>%
  dplyr::select(Location, Treatment, TrapID, Latitude, Longitude) %>%
  unique()
ggplot(coords,
       aes(Longitude, Latitude, color = Treatment)) +
  geom_point() +
  facet_wrap(~Location, scales = "free") +
  theme(aspect.ratio=1, 
        axis.text.x=element_blank(),
        axis.text.y=element_blank()) +
  ggrepel::geom_text_repel(aes(label = TrapID), color = "black", size = 3) +
  scale_x_continuous(expand = c(0.3, 0)) + 
  scale_y_continuous(expand = c(0.3, 0)) + 
  labs(title = "Trap ID's at each location")

```


## Timeline

```{r timeline, results="hide", fig.width = 7}

dates <- datR %>%
  group_by(Location, TransplantDate, TrapInstallDate) %>%
  summarize(mindate = min(SamplingDate, na.rm = T),
            maxdate = max(SamplingDate, na.rm = T)) %>%
  ungroup() %>%
  arrange(desc(mindate), Location)
dates$Location <- factor(dates$Location, levels = unique(dates$Location))

ggplot(dates) +
  geom_errorbarh(aes(xmin = mindate, xmax = maxdate, 
                     y = Location, color = Location),
                 height = 0.5, linewidth = 1.25) +
  geom_jitter(aes(TransplantDate, y = Location), width = 0, height = 0.1, size = 1.25) +
  geom_jitter(aes(TrapInstallDate, y = Location), width = 0, height = 0.1, 
              size = 1.25, color = gray(0.6)) +
  labs(y = "Location",
       x = "Date range of moths data collection (2023)",
       title = "Trapping date ranges of each location",
       caption = "Black points are transplant dates \n
       Grey points are trap installation dates") +
  scale_color_viridis_d() 
```

Note that not all traps are installed on exactly the same day; not all locations have rice transplanted on exactly the same day; and not all traps are sampled on exactly the same day.

# Basic GLM models

To start, we ignore all the spatial and temporal relationships in the data and assume each data point is independent and identically distributed (iid). **This is not a good model for the data!** It is just our starting off point.

When building hierarchical Bayesian models, it is always a good idea to start simple, make sure everything works as expected, and then build up.

## Mathematical model

Our response variable is a count, therefore we use either a Poisson or negative binomial distribution as the basis for our model. These models are known as generalized linear models (GLMs).
 
The model must also include an offset to account for the varying time intervals between sample. (If the traps are left unchecked for more days, we naturally expect the trap to have more moths. This varying effort is accounted for by the offset.)

### Poisson regression model

We start with a poisson version of our regression model:

$$
y_i \sim Pois(\lambda_i) \\
log(\lambda_i ) = \beta_0 + \beta_1 x_{A,i} + \mbox{ln} \left(DaysOfCatch_i\right)
$$

where, for data samples $i = 1, ..., N$, 

$y_i$ is moth count $i$,

$\lambda_i$ is the expected moth count for sample $i$,

$\beta_0$ is the intercept of the model. Here, it is the basis for the expected moth count for our control treatment.

$\beta_1$ is the expected difference in moth counts between the control group and the treatment A group, 

$x_{i}$ is an indicator variable that equals 1 if sample $i$ is from a treatment A field and equals 0 otherwise, and

$\left(DaysOfCatch_i\right)$ is the offset to account for the varying time interval between samples. This is necessary because with a longer time interval, more moths will fly into the trap.

### Negative binomial regression model

In ecology, there is always additional variability in our data than what the Poisson distribution allows. To account for the additional variability in our model, we switch to a negative binomial distribution:

$$
y_i \sim NegBinom(\lambda_i, \phi) \\
log(\lambda_i ) = \beta_0 + \beta_1 x_{i} + \mbox{ln} \left(DaysOfCatch_i\right)
$$

"Negative binomial regression is used to model count data for which the variance is higher than the mean." (https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-negative-binomial-regression.html)

For the negative binomial regression, all variables and parameters are defined as before, but the model has an additional parameter, $\phi$, that allows for the additional variation. The negative binomial distribution and regression model can be written in many ways. In our models, $\phi$ is defined through the following formula where the variance associated with an expected moth count, $\lambda$, is:

$$
Var(\lambda) = \lambda + \phi \cdot\lambda^2
$$

(For a Poisson distribution, $Var(\lambda) = \lambda$.)

### Bayesian mathematical model

I also fit these models using a Bayesian framework, again to build our foundation for the more complex models that come later.

For the Bayesian models, I only show the more complex negative binomial version.

To make our models Bayesian, we add priors to our parameters:

$$
y_i \sim NegBinom(\lambda_i, \phi) \\
log(\lambda_i ) = \beta_0 + \beta_1 x_{A,i} + \mbox{ln} \left(DaysOfCatch_i\right) \\
\beta_0 \sim Normal(0, 2) \\
\beta_1 \sim Normal(0, 2) \\
\phi \sim HalfCauchy(0,1)
$$

The models use slightly informative priors. When working on a log-scale (e.g., with Poisson or negative binomial distributions), this often becomes essential to avoid parameter estimates on the boundaries. (See Hooten and Hobbs for a good discussion on this issue.)

### Trapping reduction defined

The primary metric of interest from these models is a derived parameter that we call trapping reduction (TR):

$$
TR = 100 - 100 e ^{(-\beta_1)}
$$

To obtain a 95% confidence interval for TR (for the frequentist version of our model), we use the following approximation:

$$
TR = 100 - 100 e ^{(-\beta_1 \pm 2 \cdot SE(\beta_1))}
$$

For the Bayesian version, the derived parameter is sampled as part of the MCMC iterations, and the 95% credible interval is the 2.5%, 97.5% quantiles of its resulting distribution.

## Fitting the frequentist models

Note: when I predict for new data, I set DaysOfCatch = 1, and then I compare to the moths per day variable (mothsperday = nYSB / DaysOfCatch). I want to exclude any patterns related to the varying time intervals.

These models show a very tight confidence intervals for our predictions. But also, the residual deviance is MUCH greater than the degree of freedom, indicating a lack of fit. The plot of the predictions overlaid on the data also demonstrate the lack of fit.

```{r pois1}

mod1p <- glm(nYSB ~ TreatmentF, 
             offset = log(DaysOfCatch),
             family = poisson,
             data = datR)
summary(mod1p)
getTR(mod1p)
tmp = predict(mod1p, se = T, type = "link",
              newdata = datR  %>% mutate(DaysOfCatch = 1))
# str(tmp)
preds1p <- data.frame(datR, 
                      preds = tmp$fit,
                      lowerCI = tmp$fit + 2*tmp$se.fit,
                      upperCI = tmp$fit - 2*tmp$se.fit)
preds1p %<>%
  mutate(preds = exp(preds),
         lowerCI = exp(lowerCI),
         upperCI = exp(upperCI))

ggplot(preds1p,
       aes(DATI, preds, color = TreatmentF)) +
  geom_line() +
  geom_ribbon(aes(ymin = lowerCI, ymax = upperCI, fill = TreatmentF),
              alpha = 0.3) + 
  geom_point(data = mean_cts,
             aes(DATI, mean_mothsperday, color = TreatmentF), size = 1.1) +
  geom_smooth(se=F) +
  facet_wrap(~Location, scales = "free_y") + 
  scale_color_manual(values = mycolors[]) +
  scale_fill_manual(values = mycolors[]) +
  labs(title = "Poisson GLM predictions",
       subtitle = "Lines are predictions, points are the data")

```

The model is such a bad fit to the data, it is hard to even tell what is going on in the plot above.

For the negative binomial model, our confidence intervals are a little wider, but we are still ignoring all the correlations in our data. And the plot of the predictions again indicates the lack of fit.

```{r nb1}

mod1nb <- glm.nb(nYSB ~ TreatmentF + 
             offset(log(DaysOfCatch)),
             data = datR)
summary(mod1nb)
getTR(mod1nb)
tmp = predict(mod1nb, se = T, type = "link",
              newdata= datR  %>% mutate(DaysOfCatch = 1))
# str(tmp)
preds1nb <- data.frame(datR, 
                      preds = tmp$fit,
                      lowerCI = tmp$fit - 2*tmp$se.fit,
                      upperCI = tmp$fit + 2*tmp$se.fit)
preds1nb %<>%
  mutate(preds = exp(preds),
         lowerCI = exp(lowerCI),
         upperCI = exp(upperCI))
# str(preds1nb)

ggplot(preds1nb,
       aes(DATI, preds, color = TreatmentF)) +
  geom_ribbon(aes(ymin = lowerCI, ymax = upperCI, fill = TreatmentF),
              alpha = 0.3) + 
  geom_jitter(height = 0, width = 0.75, alpha = 0.5) +
  geom_point(
      aes(DATI, mothsperday, color = TreatmentF), size = 1.1) +
  geom_smooth(se=F) +
  facet_wrap(~Location, scales = "free_y") + 
  scale_color_manual(values = mycolors[]) +
  scale_fill_manual(values = mycolors[]) +
  labs(title = "Neg. binomial GLM predictions",
       subtitle = "Lines are predictions, points are the data")


  
```

## Bayesian (brms) model fit

In R, I fit the Bayesian models using the "brms" package. I find this package very intuitive and it has fast, efficient algorithms based on Stan. 

Because the models take a few minutes to fit, I usually fit them when I am initially running through my code, save them, and then only load the model fit when rendering the Rmarkdown file and creating the resulting figures.

The prior_summary command is helpful if you do not know what a parameter is called in 'brms'. Here, I used the command to find out what they called their $\phi$ parameter. (They call it the shape parameter.)

```{r, eval = F}
brm1nb <- brm(formula = nYSB3 ~ Treatment + offset(log(DaysOfCatch)),
              data = tmp, 
              family = negbinomial, #log-link is default
              prior = c(set_prior("normal(0,2)", class = "b"),
                        set_prior("normal(0,2)", class = "Intercept"),
                        set_prior("cauchy(0,1)", class = "shape")), # is half-cauchy
              warmup = 500, iter = 2000,  # use more iter for final model
              seed = 5000, # so that you get the same results
              chains = nCores, cores = nCores) 
saveRDS(brm1nb, "../output/brm1nb.RDS")
```
```{r}
brm1nb <- readRDS("../output/brm1nb.RDS")
prior_summary(brm1nb)

summary(brm1nb)
summary(mod1nb)
# 
```

The Bayesian parameter estimates match very closely to the frequentist estimates. This is expected, but reassuring that we have built the correct foundation for the more complicated models to come.

```{r}

# CALC TR!

# plot predictions!!!

```

# GLMM: Random effect (RE) for locations

The first fix we make to the model is acknowledging that overall average  moth pressure varies from location to location (see Fig 1 of moth counts). To make this fix, we add a location random effect (RE) and our model becomes a generalized linear mixed-effects model (GLMM or GLMER).

We also want to acknowledge that the treatment effect may vary from location to location-- sometimes we see a big difference in moth counts between control and treatment fields, and sometimes the difference is smaller. For inference though, we are only interested in the larger picture, which is the overall trapping reduction. (We are not interested in what happens at these exact locations per se, we are more interested in the average treatment effect.) Therefore, we also add a treatment random effect.

## GLMM mathematical model

I only show the Poisson version of the model, the NB version is a straightforward extension.

$$
y_{ik} \sim Pois(\lambda_{ik}) \\
log(\lambda_{ik} ) = \beta_0 + \beta_1 x_{A,i} + \gamma_{0k} x_{ik}  + \gamma_{1k} x_{i} x_{ik} + \mbox{ln} \left(DaysOfCatch_i\right) \\
\gamma_{0k} \sim Normal(0, \sigma^2_{0}) \\
\gamma_{1k} \sim Normal(0, \sigma^2_{1}) \\
$$

where, in addition to the variables and parameters defined for the GLM, we have

$k = 1,..., K=10$ represents the locations, 

$y_{ik}$ is moth count $i$ from location $k$,

$\lambda_{ik}$ is the expected moth count for sample $i$  from location $k$,

$\beta_0$ is the expected moth count for our control treatment for a new location,

$\beta_1$ is the expected difference in moth counts between the control group and the treatment A group for a new location, 

$\gamma_{0k}$ is the random intercept associated with location $k$, which leads to different background moth pressures at each location. All $\gamma_{0k}$ come from an iid Normal distribution.

$\gamma_{1k}$ is the random slope associated with location $k$, which leads to different treatment effects at each location. All $\gamma_{1k}$ come from an iid Normal distribution, and

$x_{ik}$ is an indicator variable that equals 1 if moth count $i$ is associated with location $k$ and 0 otherwise.

OK, technically I should be introducing matrices here because $k = 1,..., K=10$ locations, so we need a new indicator variable for each location, but I'm going to be lazy and write it like this for now.


### Bayesian GLMM

Here is the same model with a Bayesian framework and using a negative binomial regression:

To make our models Bayesian, we add priors to our parameters:

$$
y_{ik}\sim NegBinom(\lambda_{ik}, \phi) \\
log(\lambda_{ik} ) = \beta_0 + \beta_1 x_{A,i} + \gamma_{0k} x_{ik}  + \gamma_{1k} x_{A,i} x_{ik} + \mbox{ln} \left(DaysOfCatch_i\right) \\ 
\gamma_{0k} \sim Normal(0, \sigma^2_{0}) \\
\gamma_{1k} \sim Normal(0, \sigma^2_{1}) \\
\beta_0 \sim Normal(0, 2) \\
\beta_1 \sim Normal(0, 2) \\
\sigma^2_{0} \sim HalfCauchy(0,1) \\
\sigma^2_{1} \sim HalfCauchy(0,1) \\
\phi \sim HalfCauchy(0,1) \\
$$


### Bayesian GLMM -- Matrix version

I DO write the Bayesian version in matrix form because it matches the coding (and is more correct given the need for a $KxK$ $Z$ design matrix):

$$
\mathbf{Y} \sim NegBinom(\boldsymbol\lambda, \phi) \\
log(\boldsymbol\lambda) = \mathbf{X}\boldsymbol\beta + \mathbf{Z}\boldsymbol\gamma  + \mbox{ln} \left(\bf{DaysOfCatch}\right) \\
\boldsymbol\beta \sim Normal(\mathbf{0}, 2\mathbf{I}) \\
\boldsymbol\gamma \sim Normal(\mathbf{0}, \boldsymbol\sigma^2 \mathbf{I}) \\
\boldsymbol\sigma^2 \sim HalfCauchy(\mathbf{0}, 1\mathbf{I})) \\
\phi \sim HalfCauchy(0,1)
$$


## GLMM frequentist fit

A couple of notes here. R always gets mad when you ask for SE's for predictions from a GLMM. Technically, you need to run simulations to get them and then they still come with an asterisk related to their reliability. (This is a reason to use the Bayesian model-- credible intervals are never based on approximations!)

When we plot our predictions, we see that we now have better estimates for the overall mean at each location, and we see how much they vary from location to location, but there is a strong temporal pattern at each location that we are missing.

```{r pois2}
modp2 <- glmer(nYSB ~ TreatmentF + (1 + TreatmentF|Location), 
             offset = log(DaysOfCatch),
             family = poisson,
             data = datR)
summary(modp2)
getTR(modp2)
tmp = predict(modp2, se = T, type = "link",
              newdata=datR %>% mutate(DaysOfCatch = 1))
# str(tmp)
predsp2 <- data.frame(datR %>% mutate(DaysOfCatch = 1), 
                      preds = tmp$fit,
                      lowerCI = tmp$fit + 2*tmp$se.fit,
                      upperCI = tmp$fit - 2*tmp$se.fit)
predsp2 %<>%
  mutate(preds = exp(preds),
         lowerCI = exp(lowerCI),
         upperCI = exp(upperCI))
# str(predsp2)
ggplot(predsp2,
       aes(DATI, preds, color = TreatmentF)) +
  geom_line() + 
  geom_ribbon(aes(ymin = lowerCI, ymax = upperCI, fill = TreatmentF),
              alpha = 0.3) + 
  geom_point(aes(DATI, mothsperday, color = TreatmentF), size = 1.1) +
  # geom_smooth(se=F) +
  facet_wrap(~Location, scales = "free_y") + 
  scale_color_manual(values = mycolors[]) +
  scale_fill_manual(values = mycolors[])  +
  labs(title = "Poisson GLMM predictions",
       subtitle = "Lines are predictions, points are the data")

```
```{r nb2}
modnb2 <- glmer.nb(nYSB ~ TreatmentF + (1 + TreatmentF|Location), 
             offset = log(DaysOfCatch),
             data = datR)
summary(modnb2)
getTR(modnb2)
tmp = predict(modnb2, se = T, type = "link",
              newdata=datR %>% mutate(DaysOfCatch = 1))
# str(tmp)
predsnb2 <- data.frame(datR, 
                      preds = tmp$fit,
                      lowerCI = tmp$fit + 2*tmp$se.fit,
                      upperCI = tmp$fit - 2*tmp$se.fit)
predsnb2 %<>%
  mutate(preds = exp(preds),
         lowerCI = exp(lowerCI),
         upperCI = exp(upperCI))
# str(predsnb2)
ggplot(predsnb2,
       aes(DATI, preds, color = TreatmentF)) +
  geom_line() + 
  geom_ribbon(aes(ymin = lowerCI, ymax = upperCI, fill = TreatmentF),
              alpha = 0.3) + 
  geom_point(aes(DATI, mothsperday, color = TreatmentF), size = 1.1) +
  facet_wrap(~Location, scales = "free_y") + 
  scale_color_manual(values = mycolors[]) +
  scale_fill_manual(values = mycolors[])  +
  labs(title = "Neg. binomial GLMM predictions",
       subtitle = "Lines are predictions, points are the data")

```


## GLMM Bayes (brms) version

By default, the prior distributions of random slopes and intercepts are correlated in "brm". If the correlation is non-significant, you may want to remove that correlation to simplify your model. To remove the correlation, change the random effects term from "(1 + TreatmentF|Location)" to " (1 + TreatmentF||Location)" (has an extra vertical line), which makes them independent. 

You'll notice that the parameter estimates are slightly different from the frequentist NB model fit here-- the more complicated your model is, the more likely you will find this is true with slightly different versions of your model (here, adding priors and fitting with a different algorithm). 

```{r, eval = F}
brm2nb <- brm(formula = nYSB ~ TreatmentF + offset(logDaysOfCatch) + 
                    (1 + TreatmentF|Location),
              data = datR, 
              family = negbinomial,
              prior = c(set_prior("normal(0,2)", class = "b"),
                        set_prior("cauchy(0,1)", class = "shape"), 
                        set_prior("normal(0,2)", class = "Intercept"),
                        set_prior("cauchy(0,1)", class = "sd")),
              control = list(adapt_delta = 0.9), # b/c of divergent warning
              warmup = 500, iter = 2000, 
              seed = 5000,
              chains = nCores, cores = nCores) 
saveRDS(brm2nb, "../output/brm2nb.RDS")
```
```{r}
brm2nb <- readRDS("../output/brm2nb.RDS")
prior_summary(brm2nb) # we did not change the default prior for the RE correlation
# 
summary(brm2nb)
summary(modnb2)

```



# GLMM: RE for location, nested SamplingDate

In this version of the model, we acknowledge that sampling on different days of the season adds to the variability of the moth counts, and that the sampling date variability is different for each trial location. In this model, however,  the correlation between sampling dates is ignored.

This model is included here because it is helpful to think whether you have nested or crossed random effects (if applicable). And it allows us to include samplign date in our model as a random effect and still fit the model quickly in a frequentist framework.

Here, the predictions now mimc the spatial patterns we see over time, but our predictions are overly optimistic because we are ignoring the fact that the samples are actually correlated over time. (A priori, we expect that is moth counts are high on one sampling occasion, they will also be high on the next sampling occasion.)

## Nested GLMM mathematical model

Model to be filled in.


### Bayesian nested GLMM

Model to be filled in.

## Nested GLMM frequentist fit

```{r pois3, results = "hide"}
modp3 <- glmer(nYSB ~ TreatmentF  +
                     (1 + TreatmentF|Location:SamplingDateC), 
             offset = log(DaysOfCatch),
             family = poisson,
             data = datR)
summary(modp3)
getTR(modp3)
tmp = predict(modp3, se = T, type = "link",
              newdata=datR %>% mutate(DaysOfCatch = 1))
# str(tmp)
predsp3 <- data.frame(datR %>% mutate(DaysOfCatch = 1), 
                      preds = tmp$fit,
                      lowerCI = tmp$fit + 2*tmp$se.fit,
                      upperCI = tmp$fit - 2*tmp$se.fit)
predsp3 %<>%
  mutate(preds = exp(preds),
         lowerCI = exp(lowerCI),
         upperCI = exp(upperCI))
# str(predsp3)
ggplot(predsp3,
       aes(DATI, preds, color = TreatmentF)) +
  geom_line() + 
  geom_ribbon(aes(ymin = lowerCI, ymax = upperCI, fill = TreatmentF),
              alpha = 0.3) + 
  geom_point(aes(DATI, mothsperday, color = TreatmentF), size = 1.1) +
  # geom_smooth(se=F) +
  facet_wrap(~Location, scales = "free_y") + 
  scale_color_manual(values = mycolors[]) +
  scale_fill_manual(values = mycolors[])  +
  labs(title = "Poisson nested GLMM predictions",
       subtitle = "Lines are predictions, points are the data")
```
```{r nb3, results = "hide"}
# too slow!
modnb3 <- glmer.nb(nYSB ~ TreatmentF  +
                     (1+TreatmentF|Location:SamplingDateC),
             offset = log(DaysOfCatch),
             data = datR)
summary(modnb3)
getTR(modnb3)

tmp = predict(modnb3, se = T, type = "link",
              newdata=datR %>% mutate(DaysOfCatch = 1))
# str(tmp)
predsnb3 <- data.frame(datR %>% mutate(DaysOfCatch = 1), 
                       preds = tmp$fit,
                       lowerCI = tmp$fit + 2*tmp$se.fit,
                       upperCI = tmp$fit - 2*tmp$se.fit)
predsnb3 %<>%
  mutate(preds = exp(preds),
         lowerCI = exp(lowerCI),
         upperCI = exp(upperCI))
# str(predsnb2)
ggplot(predsnb3,
       aes(DATI, preds, color = TreatmentF)) +
  geom_line() + 
  geom_ribbon(aes(ymin = lowerCI, ymax = upperCI, fill = TreatmentF),
              alpha = 0.3) + 
  geom_point(aes(DATI, mothsperday, color = TreatmentF), size = 1.1) +
  # geom_smooth(se=F) +
  facet_wrap(~Location, scales = "free_y") + 
  scale_color_manual(values = mycolors[]) +
  scale_fill_manual(values = mycolors[])  +
  labs(title = "Neg. binomial nested GLMM predictions",
       subtitle = "Lines are predictions, points are the data")

```


## Nested GLMM Bayes (brms) version

```{r, eval = F}
brm3nb <- brm(formula = nYSB ~ TreatmentF + offset(log(DaysOfCatch)) + 
                    (1 + TreatmentF|Location:SamplingDateC),
              data = datR, 
              family = negbinomial,
              prior = c(set_prior("normal(0,2)", class = "b"),
                        set_prior("cauchy(0,1)", class = "shape"), # == half-cauchy
                        set_prior("normal(0,2)", class = "Intercept"),
                        set_prior("cauchy(0,1)", class = "sd")),
              warmup = 500, iter = 2000, 
              seed = 5000,
              chains = nCores, cores = nCores) 
saveRDS(brm3nb, "../output/brm3nb.RDS")
```
```{r}
brm3nb <- readRDS("../output/brm3nb.RDS")
prior_summary(brm3nb, all = FALSE)
print(prior_summary(brm3nb, all = FALSE), show_df = FALSE)

summary(brm3nb)
summary(modnb3)
# 
```

# GLMM with Gaussian Processes: RE for Trial, GP for Date

Now we are in the territory where we have to fit the models in a Bayesian framework. If our response variable was continuous (i.e., the regression model was based on a Normal distribution), then we could still use maximum likelihood estimation.

## Gaussian Process Bayes (brms) version

* Use numeric version of sampling date for better algorithm stability. Still, we have a warning about a divergent transition. Ideally, we would tweak the model and look at the data carefully so fix this warning, but because we are working with made up data, we will not worry abotu the warning for now.

```{r, eval =F}
summary(datR$numericDate) # check max before fitting, in case seasons don't line up
brm4nb <- brm(formula = nYSB ~ TreatmentF + offset(log(DaysOfCatch)) + 
                    (1 + TreatmentF||Location) + 
              gp(numericDate, by =Location), 
                  control = list(adapt_delta = 0.9),
              data = datR, 
              family = negbinomial,
              prior = c(set_prior("normal(0,2)", class = "b"),
                        set_prior("cauchy(0,1)", class = "shape"), # == half-cauchy
                        set_prior("normal(0,2)", class = "Intercept"),
                        set_prior("cauchy(0,1)", class = "sd"),
                        set_prior("cauchy(0,1)", class = "lscale", 
                                  coef = "gpnumericDateLocationLoc1"),
                        set_prior("cauchy(0,1)", class = "lscale", 
                                  coef = "gpnumericDateLocationLoc2"),
                        set_prior("cauchy(0,1)", class = "lscale", coef = "gpnumericDateLocationLoc3"),
                        set_prior("cauchy(0,1)", class = "lscale", coef = "gpnumericDateLocationLoc4"),
                        set_prior("cauchy(0,1)", class = "lscale", coef = "gpnumericDateLocationLoc5"),
                        set_prior("cauchy(0,1)", class = "lscale", coef = "gpnumericDateLocationLoc6"),
                        set_prior("cauchy(0,1)", class = "lscale", coef = "gpnumericDateLocationLoc7"),
                        set_prior("cauchy(0,1)", class = "lscale", coef = "gpnumericDateLocationLoc8"),
                        set_prior("cauchy(0,1)", class = "lscale", coef = "gpnumericDateLocationLoc9"),
                        set_prior("cauchy(0,1)", class = "lscale", coef = "gpnumericDateLocationLoc10"),
                        set_prior("normal(0,2)", class = "sdgp")),
                        # set_prior("lkj(2)", class = "cor")),
              warmup = 1000, iter = 5000, 
              seed = 5000,
              chains = nCores, cores = nCores) 
saveRDS(brm4nb, "../output/brm4nb.RDS")
```
```{r}
brm4nb <- readRDS("../output/brm4nb.RDS")
summary(brm4nb)
bayes_R2(brm4nb) # 0.824

prior_summary(brm4nb)
prior_summary(brm4nb, all = FALSE)
# 
```





